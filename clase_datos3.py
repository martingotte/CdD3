# -*- coding: utf-8 -*-
"""clase_datos3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15fbg-y0F_CIJoMYKrBmniviwWznrPxLi
"""

"""
modulo: clase_datos3
--------------------

Este modulo proporciona clases para realizar resumen estadistico, generacion de datos, estimacion de densidades, anova, regresion simple, multiple y logistica.

Clases:
    GeneradoraDeDatos: produce datos de distintas distribuciones.

    Estimacion: proporciona calculos de estadisticos basicos.

    Regresion: clase madre de regresion.

    RegresionLineal: contiene funciones de regresion lineal simple y multiple, hereda funciones de Regresion.

    RegresionLogistica: contiene funciones de regresion logistica, hereda funciones de Regresion.

    Anova: muestra resultados de un analisis de anova.

    cualitativas: compara muestras con distribuciones usando chi-cuadrado.

Funciones:
    dummies: convierte variables categoricas en dummies.

"""

#librerias usadas:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from scipy import stats
import random
from sklearn.metrics import auc
from statsmodels.stats.anova import anova_lm



class GeneradoraDeDatos:
  """
  clase para generar datos, usando numpy y scipy.stats

  atributos:


  """
  def __init__(self,N=1000):
    self.N=N

    """
    inicializa una instancia de GeneradoraDeDatos.

    Args:
      N(float): numero de datos a generar
    """

  def distribucion_datos_normal(self,media=0,desvio=1) -> np.ndarray:
    """
    simula una distribuacion teorica normal.

    Args:
      media(float): media de la distribucion normal.
      desvio(float): desvio de la distribucion normal.

    """
    __grilla=np.linspace(media-3*desvio,media+3*desvio,self.N)
    self.datos_normal=self.pdf_norm(__grilla,media,desvio)
    return self.datos_normal

  def generar_datos_normal(self,media=0,desvio=1) -> np.ndarray:
    """
    genera una muestra normal random.

    Args:
      media(float): media de la distribucion normal.
      desvio(float): desvio de la distribucion normal.

    """
    return np.random.normal(media,desvio,self.N)

  def distribucion_datos_uniforme(self,min,max) -> np.ndarray:
    """
    simula una distribucion teorica uniforme.

    Args:
      min: valor minimo de la distribucion.
      max: valor maximo de la distribucion.

    """
    __grilla=np.linspace(min,max,self.N)
    self.datos_uniforme=self.pdf_uniforme(__grilla,min,max)
    return self.datos_uniforme

  def generar_datos_uniforme(self,min,max) -> np.ndarray:
    """
    genera una muestra uniforme random.

    Args:
      min: valor minimo de la distribucion.
      max: valor maximo de la distribucion.

    """
    return np.random.uniform(min,max,self.N)

  def pdf_uniforme(self,x,min,max) -> np.ndarray:
    """
    devuelve la densidad de x en la distribucion uniforme.

    Args:
      x: valor de entrada.
      min: valor minimo de la distribucion.
      max: valor maximo de la distribucion.
    """
    return stats.uniform.pdf(x,min,max)

  def cdf_uniforme(self,x,min,max):
    """
    devuelve la densidad acumulada de x en la distribucion uniforme.

    Args:
      x: valor de entrada.
      min: valor minimo de la distribucion.
      max: valor maximo de la distribucion.
    """
    return stats.uniform.cdf(x,min,max)

  def ppf_uniforme(self,x,min,max) -> np.ndarray:
    """
    devuelve el percentil correspondiente a x de la distribucion uniforme.

    Args:
      x: valor de entrada.
      min: valor minimo de la distribucion.
      max: valor maximo de la distribucion.
    """
    return stats.uniform.ppf(x,min,max)

  def generar_datos_BS(self) -> np.ndarray:
    """
    genera una muestra de distribucion BS(Bart Simpson).
    """
    u = np.random.uniform(size=(self.N,))
    y = u.copy()
    ind = np.where(u > 0.5)[0]
    y[ind] = np.random.normal(0, 1, size=len(ind))
    for j in range(5):
        ind = np.where((u > j * 0.1) & (u <= (j+1) * 0.1))[0]
        y[ind] = np.random.normal(j/2 - 1, 1/10, size=len(ind))
    return y

  def distribucion_datos_BS(self,min,max) -> np.ndarray:
    """
    simula una distribucion teorica BS(Bart Simpson).

    Args:
      min: valor minimo de la distribucion.
      max: valor maximo de la distribucion.
    """
    grilla=np.linspace(min,max,self.N)
    sumatoria=0
    for j in range(5):
      sumatoria+=stats.norm.pdf(grilla,(j/2)-1,1/10)
    self.datos_BS=(1/2)*stats.norm.pdf(grilla,0,1)+(1/10)*sumatoria
    return grilla,self.datos_BS

  def generar_datos_exponencial(self,b) -> np.ndarray:
    """
    genera una muestra exponencial random.

    Args:
      b: parametro de la distribucion exponencial.
    """
    return np.random.exponential(scale=b,size=self.N)




class Estimacion:
  def __init__(self, datos):
    self.datos=datos

  def media(self):
    self.media=np.mean(self.datos)
    return self.media

  def mediana(self):
    self.mediana=np.median(self.datos)
    return self.mediana

  def desvio_estandar(self):
    self.desvio=np.std(self.datos)
    return self.desvio

  def varianza(self):
    self.varianza=np.var(self.datos)
    return self.varianza

  def cuartiles(self):
    self.q1=np.percentile(self.datos,25)
    self.q2=np.percentile(self.datos,50)
    self.q3=np.percentile(self.datos,75)
    return [self.q1,self.q2,self.q3]

  def maximo(self):
    maximo=np.max(self.datos)
    return maximo

  def minimo(self):
    minimo=np.min(self.datos)
    return minimo

  def calcular_histograma(self,h):
    puntos=np.arange(np.min(self.datos),np.max(self.datos)+h,h)
    histograma=np.zeros(len(puntos)-1)
    for i in range(len(histograma)):
      for j in range(len(self.datos)):
        if self.datos[j]>=puntos[i] and self.datos[j]<puntos[i+1]:
          histograma[i]+=1
      histograma[i]=histograma[i]/len(self.datos)
      histograma[i]=histograma[i]/h
    return histograma,puntos

  def evalua_histograma(self,h,x):
    histograma,puntos=self.calcular_histograma(h)
    estim_hist=np.zeros(len(x))
    for i in range(len(x)):
      for j in range(len(puntos)-1):
        if x[i]>=puntos[j] and x[i]<puntos[j+1]:
          estim_hist[i]=histograma[j]
    return estim_hist

  def kernel_uniforme(self,x):
    if (x>=-1/2) and (x<=1/2):
      valor_kernel_uniforme=1
    else:
      valor_kernel_uniforme=0
    return valor_kernel_uniforme

  def kernel_gaussiano(self,x):
    valor_kernel_gaussiano=1/np.sqrt(2*np.pi)*np.exp(-x**2/2)
    return valor_kernel_gaussiano

  def kernel_cuadratico(self,x):
    if (x>=-1/2) and (x<=1/2):
      valor_kernel_cuadratico=(3/4)*(1-x**2)
    else:
      valor_kernel_cuadratico=0
    return valor_kernel_cuadratico

  def kernel_triangular(self,x):
    valor_kernel_triangular=(1+x)*(x>=0 and x<=1)+(1-x)*(x<0 and x>-1)
    return valor_kernel_triangular

  def densidad_nucleo(self,h,kernel,x):
    if kernel=="uniforme":
      density=np.zeros_like(x,dtype=float)
      for i in range(len(x)):
        for j in range(len(self.datos)):
          density[i]+=self.kernel_uniforme((self.datos[j]-x[i])/h)
      density=density/(len(self.datos)*h)
    elif kernel=="gaussiano":
      density=np.zeros_like(x,dtype=float)
      for i in range(len(x)):
        for j in range(len(self.datos)):
          density[i]+=self.kernel_gaussiano((self.datos[j]-x[i])/h)
      density=density/(len(self.datos)*h)
    elif kernel=="cuadratico":
      density=np.zeros_like(x,dtype=float)
      for i in range(len(x)):
        for j in range(len(self.datos)):
          density[i]+=self.kernel_cuadratico((self.datos[j]-x[i])/h)
      density=density/(len(self.datos)*h)
    elif kernel=="triangular":
      density=np.zeros_like(x,dtype=float)
      for i in range(len(x)):
        for j in range(len(self.datos)):
          density[i]+=self.kernel_triangular((self.datos[j]-x[i])/h)
      density=density/(len(self.datos)*h)
    return density

  def EMC_normal(self,datos_teoricos):
    __grilla_h=np.linspace(0.01,5,100)
    __grilla_x=np.linspace(-5,5,300)
    errores=[]
    for h in __grilla_h:
      datos_hist=self.evalua_histograma(h,__grilla_x)
      errores.append(np.sum((datos_hist-datos_teoricos)**2)/len(datos_teoricos))
    h_minimo=__grilla_h[np.argmin(errores)]
    EMC=np.min(errores)
    return h_minimo,EMC

  def EMC_kernel(self,datos_teoricos,kernel):
    __grilla_h=np.linspace(0.01,5,100)
    __grilla_x=np.linspace(-5,5,100)
    errores=[]
    for h in __grilla_h:
      datos_hist=self.densidad_nucleo(h,kernel,__grilla_x)
      errores.append(np.sum((datos_hist-datos_teoricos)**2)/len(datos_teoricos))
    h_minimo=__grilla_h[np.argmin(errores)]
    EMC=np.min(errores)
    return h_minimo,EMC

  def pdf_norm(self,media=0,desvio=1):
    return stats.norm.pdf(self.datos,media,desvio)

  def ppf_norm(self,media=0,desvio=1):
    return stats.norm.ppf(self.datos,media,desvio)

  def cdf_norm(self,media=0,desvio=1):
    return stats.norm.cdf(self.datos,media,desvio)

  def pdf_uniforme(self,min,max):
    return stats.uniform.pdf(self.datos,min,max)

  def cdf_uniforme(self,min,max):
    return stats.uniform.cdf(self.datos,min,max)

  def ppf_uniforme(self,min,max):
    return stats.uniform.ppf(self.datos,min,max)

  def miqqplot(self):
    n=len(self.datos)
    cuantiles_teoricos=self.ppf_norm(np.arange(1/(n+1),1,1/(n+1)))
    datos_ordenados=np.sort(self.datos)
    cuantiles_muestrales=((datos_ordenados-np.mean(datos_ordenados))/np.std(datos_ordenados))
    plt.scatter(cuantiles_teoricos, cuantiles_muestrales, color='blue', marker='o')
    plt.xlabel('Cuantiles te√≥ricos')
    plt.ylabel('Cuantiles muestrales')
    plt.plot(cuantiles_teoricos,cuantiles_teoricos , linestyle='-', color='red')
    plt.show()

def dummies(datos,columna):
    dummies=pd.get_dummies(datos[columna],columns=[columna],drop_first=True).astype(int)
    return dummies




class Regresion():
  def __init__(self,x,y):
    self.x=x
    self.y=y

  def separar_datos(self,porcentaje=.8,semilla=10):
    n = self.y.shape[0]
    n_train=int(n*0.8)
    n_test=n-n_train
    random.seed(semilla)
    cuales = random.sample(range(n), n_train)
    self.y_test = self.y.drop(cuales)
    self.x_test = self.x.drop(cuales)
    self.y_train = self.y.iloc[cuales]
    self.x_train = self.x.iloc[cuales]




class RegresionLineal(Regresion):
  def __init__(self,x,y):
    super().__init__(x,y)
    self.modelo=sm.OLS(self.y,self.x)
    self.resultados=self.modelo.fit()

  def resultados(self):
    print(self.resultados.summary())

  def parametros(self):
    return self.resultados.params()

  def p_valores(self):
    return self.resultados.pvalues()

  def obtener_residuos(self):
    return self.resultados.resid

  def varianza_residuos(self):
    return self.resultados.mse_resid

  def desvio_estimado(self):
    return self.resultados.bse()

  def t_obs(self):
    return self.resultados.tvalues()

  def int_confianza_b(self):
    return self.resultados.conf_int()

  def r_squared(self):
    return self.resultados.rsquared()

  def r_squared_adj(self):
    return self.resultados.rsquared_adj()

  def prediccion(self,x_new):
    prediccion=self.resultados.predict(x_new)
    return prediccion

  def predicciones(self,x_new):
    prediccion=self.resultados.get_prediction(x_new)
    return prediccion





class RegresionLogistica(Regresion):
  """
  clase para hacer calculos de regresion logistica

  Args:
    Regresion: clase heredada.
  """
  def __init__(self,x,y):
    super().__init__(x,y)
    self.modelo=sm.Logit(self.y,self.x)
    self.ajuste=self.modelo.fit()

  def resultados(self):
    print(self.ajuste.summary())

  def parametros(self):
    return self.ajuste.params

  def p_valores(self):
    return self.ajuste.pvalues

  def obtener_residuos(self):
    return self.ajuste.resid_pearson

  def desvio_estimado(self):
    return self.ajuste.bse

  def t_obs(self):
    return self.ajuste.tvalues

  def int_confianza_b(self):
    return self.ajuste.conf_int()

  def prediccion(self,x_new):
    prediccion=self.ajuste.predict(x_new)
    return prediccion

  def predicciones(self,x_new):
    prediccion=self.ajuste.get_prediction(x_new)
    return prediccion.summary_frame()

  def test(self,porcentaje=.8,semilla=10,punto_corte=0.5):
    """
    funcion para calcular el error total, la sensibilidad y especificidad de un modelo de regresion logistica.

    Args:
      porcentaje: porcentaje de datos que iran a la instancia de entrenamiento.
      semilla: seed por si se requiere  unificar resultados.
      punto_corte: valor de corte para la prediccion de las respuestas.
    """
    super().separar_datos(porcentaje,semilla) #separa los datos en train y test, hereda la funcion de Regresion
    regresion_train=RegresionLogistica(self.x_train,self.y_train) #instancia la misma clase con los datos de entrenamiento
    parametros_train=regresion_train.parametros() #guarda los betas
    pi_test=[] #inicializa lista para las probabilidades de exito del test.
    for i in range(len(self.x_test)): #recorre el rango de x_test
      pred=regresion_train.prediccion(self.x_test.iloc[[i]]) #predice el valor de y de los x_test a partir del modelo de entrenamiento
      pi_test.append(pred.item()) #agregamos el valor de predicicon
    self.y_pred=[] #inicializamos una lista para las respuestas predichas
    for i in range(len(pi_test)): #recorremos el rango de pi_test
      if pi_test[i]<punto_corte: #verificamos si las probabilidades son mayores que el punto de corte y agregamos un 1 si si y un 0 si no.
        self.y_pred.append(0)
      else:
        self.y_pred.append(1)
    self.y_pred=np.array(self.y_pred) #transformamos la lista en arreglo
    self.error=np.mean(self.y_pred!=self.y_test.values) #calculamos error
    self.sensibilidad=(np.sum((self.y_pred==1)&(self.y_test==1)))/np.sum(self.y_test==1) #calculamos sensibilidad
    self.especificidad=(np.sum((self.y_pred==0)&(self.y_test==0)))/np.sum(self.y_test==0) #calculamos especificidad
    return self.error,self.sensibilidad,self.especificidad

  def indice_youden(self,porcentaje=.8,semilla=10):
    grilla_corte=np.linspace(0,1,100)
    lista_youden=[]
    self.lista_sensibilidad=[]
    self.lista_especificidad=[]
    for i in range(len(grilla_corte)):
      error,sensibilidad,especificidad=self.test(porcentaje,semilla,punto_corte=grilla_corte[i])
      lista_youden.append(sensibilidad+especificidad-1)
      self.lista_sensibilidad.append(sensibilidad)
      self.lista_especificidad.append(especificidad)
    indice_youden=np.argmax(lista_youden)
    return grilla_corte[indice_youden],self.lista_sensibilidad[indice_youden],self.lista_especificidad[indice_youden]

  def AUC(self):
    __instancia=RegresionLogistica(self.x,self.y)
    _,_,_=__instancia.indice_youden()
    sensibilidad=np.array(__instancia.lista_sensibilidad)
    especificidad=np.array(__instancia.lista_especificidad)
    return auc(1-especificidad,sensibilidad)



class anova():
  def __init__(self, resultados_reducidos, resultados):
    self.resultados = resultados
    self.resultados_reducidos = resultados_reducidos
    self.ajuste=anova_lm(self.resultados_reducidos,self.resultados)

  def test(self):
    return self.ajuste

  def p_valores(self):
    return self.anova.iloc[:,5]

  def f_valores(self):
    return self.anova.iloc[:,4]





class cualitativas:
  """
  clase para determinar si una distribucion no sigue una cierta distribucion usando chi-cuadrado.

  """
  def __init__(self,muestra,muestra_teorica):
    """
    inicializa una instancia de cualitativas.

    Args:
      muestra: muestra de datos a comparar.
      muestra_teorica: muestra teorica.
    """
    self.muestra=muestra
    self.muestra_teorica=muestra_teorica
    self.grados_libertad=len(muestra)-1 #asigna los grados de libertad: n-1 (n:cantidad de respuestas).

  def chi_observado(self):
    """
    calcula el valor de chi-cuadrado observado.
    """
    x2_observado=np.sum((self.muestra-self.muestra_teorica)**2/self.muestra_teorica)
    return x2_observado

  def chi_alpha(self,alpha=0.05):
    """
    calcula el valor de chi-cuadrado critico.

    Args:
      alpha: nivel de significancia.
    """
    x2_alpha=stats.chi2.ppf(1-alpha,self.grados_libertad)
    return x2_alpha

  def p_valor(self):
    """
    calcula el p-valor de nuestro chi observado.
    """
    x2_observado=self.chi_observado() #llamamos a la funcion chi_observado de la misma clase.
    p_valor=1-stats.chi2.cdf(x2_observado,self.grados_libertad)
    return p_valor